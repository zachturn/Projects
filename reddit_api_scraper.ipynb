{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit API Web Scraper\n",
    "I started building a true web scraper using BeautifulSoup, and later discovered Reddit has an API to grab content, so I'm going to take advantage of that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Local file that stores my credentials for accessing the API\n",
    "from reddit_oauth import username, password, client_id, client_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subreddit = 'summonerschool'\n",
    "\n",
    "client_auth = requests.auth.HTTPBasicAuth(client_id, client_secret)\n",
    "post_data = {\"grant_type\": \"password\", \"username\": username, \"password\": password,\"redirect_uri\":'http://localhost'}\n",
    "user_agent = {\"User-Agent\": \"Comment Scraper app by /u/\" + username}\n",
    "response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=user_agent)\n",
    "\n",
    "headers = {\"Authorization\": response.json()['token_type'] + \" \" + response.json()['access_token'],\n",
    "           \"User-Agent\": \"Comment Scraper app by /u/\" + username}\n",
    "\n",
    "# This is the webpage I want to scrape. For this intial scrape, I want to make sure\n",
    "# I am updating my user-agent because I'm not accessing the reddit API just yet.\n",
    "main_json = requests.get('http://www.reddit.com/r/' + subreddit + '/.json',headers=user_agent)\n",
    "\n",
    "# Create skeleton dictionary with thread titles as keys.\n",
    "master = {}\n",
    "for each in main_json.json()['data']['children']:\n",
    "    master.setdefault(each['data']['title'],{'id':each['data']['id'],'json':[],'comments':[],'more':[],'expected_comments':each['data']['num_comments'],'actual_comments':0})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_more(comment):\n",
    "    if comment['kind']=='more':\n",
    "        if comment['data']['parent_id'][0:2]==\"t3\":\n",
    "            v['more'].extend(tuple(comment['data']['children']))\n",
    "        return True\n",
    "\n",
    "def get_more_children(parent,comment):\n",
    "    more_comment_ids = comment['data']['children']\n",
    "    more_comments = requests.get('https://oauth.reddit.com/api/morechildren',headers=headers,\n",
    "                        params={'children':','.join(more_comment_ids),'link_id':link_id})\n",
    "    more_comments = more_comments.json()['jquery'][14][3][0]\n",
    "    for more in more_comments:\n",
    "        if check_more(more):\n",
    "            if more['data']['count']==0:\n",
    "                continue\n",
    "            get_more_children(parent,more)\n",
    "            continue\n",
    "        parent.append(more['data']['body'].lower())\n",
    "        get_children(parent,more)\n",
    "\n",
    "def get_children(parent,comment):\n",
    "    if comment['data']['replies'] != '':\n",
    "        children = comment['data']['replies']['data']['children']\n",
    "        for child in children:\n",
    "            if check_more(child):\n",
    "                if child['data']['count']==0:\n",
    "                    continue\n",
    "                get_more_children(parent,child)\n",
    "                continue\n",
    "            else:\n",
    "                parent.append(child['data']['body'].lower())\n",
    "                get_children(parent,child)\n",
    "            \n",
    "def get_parents(comments):\n",
    "    for each in tqdm_notebook(comments,desc='Parents',leave=False):\n",
    "        if check_more(each): \n",
    "            continue\n",
    "        parent = [each['data']['body'].lower()]\n",
    "        v['comments'].append(parent)\n",
    "        get_children(parent,each)\n",
    "\n",
    "def get_more_parents(more_parents):\n",
    "    for parent_id in tqdm_notebook(more_parents,desc='More Parents',leave=False):\n",
    "        parent_comment = requests.get(url,headers=headers,\n",
    "                                     params={'comment':parent_id,'showmore':True}) \n",
    "        parent_comments = parent_comment.json()[1]['data']['children']\n",
    "        get_parents(parent_comments)\n",
    "        time.sleep(0.2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for k,v in tqdm_notebook(master.items(),desc='Thread'):\n",
    "    \n",
    "    url = 'https://oauth.reddit.com/r/' + subreddit + '/comments/' + v['id']\n",
    "    v['json'] = requests.get(url,headers=headers,\n",
    "                    params={'showmore':True,'limit':100000})\n",
    "    \n",
    "    if v['json'].status_code!=200:\n",
    "        v['success'] = False\n",
    "    \n",
    "    link_id = v['json'].json()[0]['data']['children'][0]['data']['name']\n",
    "    comments = v['json'].json()[1]['data']['children']\n",
    "    get_parents(comments)\n",
    "    get_more_parents(v['more'])\n",
    "    \n",
    "    for comments in v['comments']:\n",
    "        v['actual_comments'] += len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Questions Simple Answers: 60 1484 / 1479\n",
      "What team comps are the following mid laners good in or against? 0 / 0\n",
      "Team Comps, and Different Ways to End? 6 / 6\n",
      "Things you're doing wrong that you probably didn't realise: Support Edition 39 / 38\n",
      "In lower ELO, why do teams group mid so fast? 38 / 38\n",
      "Weekly Replay Review Thread: Week 38 10 / 10\n",
      "When to group and when not too. 6 / 6\n",
      "Shyvana micro and skins... Hm? 0 / 0\n",
      "Chogath Feast indicator not showing in-game 5 / 5\n",
      "A guide to being an One Trick Pony 132 / 133\n",
      "wave managment 2 / 2\n",
      "Crazy idea I came upon. And I wonder if this could work? 1 / 1\n",
      "would this warwick idea work (mechanics, interactions) 6 / 6\n",
      "ADC's situation after changing botlane turrets - having problem and looking for solution 7 / 7\n",
      "Is it OK to /mute all on every single game? 32 / 32\n",
      "Where am I supposed to go as an ADC? 3 / 3\n",
      "Looking for some Shyvana tips 17 / 16\n",
      "I ban Yasuo when someone hovers it. Is it normal to do? 15 / 15\n",
      "New Free Champion Rotation 17 / 17\n",
      "Any simple carry top laners? 36 / 37\n",
      "Hey everyone, LS here, AMAA! 1247 / 1257\n",
      "How do I explain to my team that team comp doesn't matter? 3 / 1\n",
      "How do other people gain elo so fast? 7 / 7\n",
      "Help me please. [Plat 5] 7 / 7\n",
      "Your buttons and their value (for newer players) 0 / 0\n",
      "Am I holding myself back by playing Yasuo? 61 / 62\n",
      "Best Bans by Tier in Each Region (Patch 6.16): A Statistical Analysis - Ashe Edition 25 / 25\n"
     ]
    }
   ],
   "source": [
    "for k,v in master.items():\n",
    "    print(k,v['actual_comments'],\"/\",v['expected_comments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get more pages of threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get after key. This will move the subreddit to the next page\n",
    "main_json.json()['data']['after']\n",
    "test = requests.get('http://www.reddit.com/r/summonerschool/.json',headers=headers,\n",
    "                   params={'after':main_json.json()['data']['after']})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
