{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reddit API Web Scraper**\n",
    "***\n",
    "## Introduction\n",
    "Using the reddit API, you can extract not only subreddit threads but also the comments for each threads. This notebook will grab every comment for each thread for a specified number of pages. \n",
    "\n",
    "I'll create a dictionary that contains thread titles as keys, and the comments as a sub-dictionary of values. I'll store each of the comment trees in separate lists so that I can maintain some structure within each thread. Essentially, each sub-list will contain the parent comment and its replies, e.g:\n",
    "\n",
    "{'thread': {'comments': [[parent, reply, reply,...] , [parent,reply,...] , ...]}}\n",
    "\n",
    "\n",
    "## Steps\n",
    "The basic steps for this are:\n",
    "\n",
    "1. Grab the first subreddit page as a .json file (as simple as requesting www.reddit.com/r/subreddit/.json).  \n",
    "&nbsp;\n",
    "2. Take this first page, and extract thread metadata (title, number of comments, etc.) and store in the master dictionary. This master dictionary is also where the comment text will be stored. Repeat this for as many pages as desired (up to 100 max I believe).  \n",
    "&nbsp;\n",
    "3. After all thread metadata has been collected, start iterating through the threads. For each thread:\n",
    "    1. Grab all of the comment data that a user would see initially (let's call these main comments).\n",
    "    2. Take a parent comment and start a sub-list in the thread's comment key in the master dictionary.\n",
    "    3. Iterate through each child in the comment tree and append it into the sub-list.\n",
    "    4. If a child is hidden under \"load more comments\", call the API again to expand the comments.\n",
    "    5. Repeat steps B-D until all of the main comments have been grabbed.\n",
    "    6. Now, there are parent comments hidden under \"load more comments\". Call the API to pull a portion of these comments, and apply the same steps that you did to the main comments.\n",
    "    7. Repeat this until all comments have been retrieved.  \n",
    "&nbsp;\n",
    "4. Save the master dictionary of threads and comments to a .json file.\n",
    "\n",
    "## API Notes\n",
    "Detailed information on the Reddit API functions can be found [here](https://www.reddit.com/dev/api/). You can find details on the API rules [here](https://github.com/reddit/reddit/wiki/API).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "***\n",
    "Below is a walk through on the code I used to perform my comment scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# I created a local file with my OAuth2 credentials, so that I can share this without giving up the information.\n",
    "from reddit_oauth import username, password, client_id, client_secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "These are the functions I created to carry about the scraping. Most of them are recursive in nature, since the API doesn't really let you perform everything in one attempt. The recursion will allow you to keep chipping away at the remaining comments until each thread is fully scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_parents(parents):\n",
    "    \"\"\"Iterate through each comment tree parent, and create a list of all comments within the tree\"\"\"\n",
    "    \n",
    "\n",
    "    for each in tqdm_notebook(parents,desc='Parents',leave=False):\n",
    "        \n",
    "        # If the parent is 'more' type, it will be appended into a separate list to be expanded later\n",
    "        if check_more(each): \n",
    "            continue\n",
    "            \n",
    "        # Start a new list for each new parent thread, and place it in the master dictionary\n",
    "        parent = [each['data']['body'].lower()]\n",
    "        v['comments'].append(parent)\n",
    "        get_children(parent,each)\n",
    "\n",
    "def get_children(parent,comment):\n",
    "    \"\"\"Given a parent tree, retrieve all of the child comments (if there are any)\"\"\"\n",
    "\n",
    "    \n",
    "    if comment['data']['replies'] != '':\n",
    "        children = comment['data']['replies']['data']['children']\n",
    "        for child in children:\n",
    "            \n",
    "            # If the child comment is hidden under 'load more comments' expand it out here\n",
    "            if check_more(child):\n",
    "                \n",
    "                # This is a special case when a tree gets particularly deep (around 10+ replies) and the comment \n",
    "                # is listed under \"Continue this thread\" to the viewer. This comment data is stored differently\n",
    "                # so it is easiest to simply skip it.\n",
    "                if child['data']['count']==0:\n",
    "                    continue\n",
    "                get_more_children(parent,child)\n",
    "                continue\n",
    "            else:\n",
    "                # If there aren't any 'more' comments, recurse through it normally.\n",
    "                parent.append(child['data']['body'].lower())\n",
    "                get_children(parent,child)\n",
    "        \n",
    "def get_more_parents(more_parents):\n",
    "    \"\"\"After creating the list of all 'more' parent comments, this function will call the API to access the comments within each of these new trees\"\"\"\n",
    "    \n",
    "    for parent_id in tqdm_notebook(more_parents,desc='More Parents',leave=False):\n",
    "        \n",
    "        # I need to make a separate API request for each individual parent\n",
    "        time.sleep(1.001)\n",
    "        parent_comment = requests.get(url,headers=headers,params={'comment':parent_id}) \n",
    "        parent_comments = parent_comment.json()[1]['data']['children']\n",
    "        get_parents(parent_comments)\n",
    "              \n",
    "\n",
    "def get_more_children(parent,comment):\n",
    "    \"\"\"Given a parent tree that contains 'more' child, expand out the 'more' children and append to the tree\"\"\"\n",
    "\n",
    "    more_comment_ids = comment['data']['children']\n",
    "    \n",
    "    # I need to make a separate API call in order to expand the 'more' comments\n",
    "    time.sleep(1.001)\n",
    "    more_comments_json = requests.get('https://oauth.reddit.com/api/morechildren',headers=headers,\n",
    "                        params={'children':more_comment_ids,'link_id':link_id})\n",
    "    more_comments = more_comments_json.json()['jquery'][14][3][0]\n",
    "    \n",
    "    for more in more_comments:\n",
    "        \n",
    "        # If the tree has a lot of replies, sometimes the first 'more' API request doesn't grab every comment\n",
    "        # and we need to call the function recursively until we retrieve them all.\n",
    "        if check_more(more):\n",
    "            if more['data']['count']==0:\n",
    "                continue\n",
    "            get_more_children(parent,more)\n",
    "            continue\n",
    "            \n",
    "        parent.append(more['data']['body'].lower())\n",
    "        get_children(parent,more)\n",
    "            \n",
    "def check_more(comment):\n",
    "    \"\"\"Take an input comment and return True if the comment type is 'more'\"\"\"\n",
    "\n",
    "    if comment['kind']=='more':\n",
    "        \n",
    "        # Comments with 'parent_id'=='t3' are parent comments, not children.\n",
    "        # These need to be sent into the ['more'] dictionary to be expanded later.\n",
    "        if comment['data']['parent_id'][0:2]==\"t3\":\n",
    "            v['more'].extend(tuple(comment['data']['children']))\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit API Credentials\n",
    "In order to use the Reddit API, you need to follow their [Oauth2 Verification Procedure](https://github.com/reddit/reddit/wiki/OAuth2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Credentials to generate my token. \n",
    "client_auth = requests.auth.HTTPBasicAuth(client_id, client_secret)\n",
    "post_data = {\"grant_type\": \"password\", \"username\": username, \"password\": password,\"redirect_uri\":'http://localhost'}\n",
    "\n",
    "# I need to modify my user agent in order to make the intial .json pull using BeautifulSoup\n",
    "user_agent = {\"User-Agent\": \"Comment Scraper app by /u/\" + username}\n",
    "\n",
    "# Generate the API access token\n",
    "response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=user_agent)\n",
    "\n",
    "# This is the header I'll use to access the API\n",
    "headers = {\"Authorization\": response.json()['token_type'] + \" \" + response.json()['access_token'],\n",
    "           \"User-Agent\": \"Comment Scraper app by /u/\" + username}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping\n",
    "Here is where most of the work actually happens. I create my master dictionary with thread metadata, and then use my scraper functions to populate the master dictionary with the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Here you can enter whichever subreddit you'd like, and however many pages you wish to scrape.\n",
    "subreddit = 'summonerschool'\n",
    "num_pages = 100\n",
    "\n",
    "master = {}\n",
    "\n",
    "# The after variable is what we'll use to step through the pages. Reddit doesn't use page numbers in their .json\n",
    "# files, so the 'after' id will allow the API to know which pages we have already scraped.\n",
    "after = None\n",
    "\n",
    "# Create the master dictionary and store the metadata.\n",
    "for pages in range(num_pages):\n",
    "    \n",
    "    # This is the webpage I want to scrape. For this intial scrape, I want to make sure\n",
    "    # I am updating my user-agent because I'm not accessing the reddit API just yet.\n",
    "    url = 'http://www.reddit.com/r/' + subreddit + '/.json'\n",
    "    main_json = requests.get(url,headers=user_agent,params={'after':after})\n",
    "    \n",
    "    # Now I update my after value so I can get a new set of threads for the next iteration\n",
    "    after = main_json.json()['data']['after']\n",
    "\n",
    "    for each in main_json.json()['data']['children']:\n",
    "        master.setdefault(each['data']['title'],{'id':each['data']['id'],'json':[],'comments':[],'more':[],'expected_comments':each['data']['num_comments'],'actual_comments':0})   \n",
    "\n",
    "# Iterate through each thread in the master dictionary, and scrape the comments\n",
    "for k,v in tqdm_notebook(master.items(),desc='Thread',leave=False):\n",
    "    \n",
    "    \n",
    "    url = 'https://oauth.reddit.com/r/' + subreddit + '/comments/' + v['id']\n",
    "    time.sleep(1.001)\n",
    "    v['json'] = requests.get(url,headers=headers)\n",
    "    \n",
    "    # This is a quality check. If the .json is scraped successfully its status code is 200, so if any of the\n",
    "    # .json files are not 200, raise an error and stop the loop.\n",
    "    if v['json'].status_code!=200:\n",
    "        print(\"JSON Retrieval Error\")\n",
    "        raise KeyboardInterrupt\n",
    "    \n",
    "    # The link_id is needed when calling the get/api/morechildren. \n",
    "    link_id = v['json'].json()[0]['data']['children'][0]['data']['name']\n",
    "    \n",
    "    # This is where the recursion begins and the comments are scraped.\n",
    "    parents = v['json'].json()[1]['data']['children']\n",
    "    get_parents(parents)\n",
    "    get_more_parents(v['more'])\n",
    "    \n",
    "    # Store number of comments retrieved in each thread to compare to expected number.\n",
    "    for comments in v['comments']:\n",
    "        v['actual_comments'] += len(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Verification\n",
    "***\n",
    "I created just a few attributes to check to examine the quality of the scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 954 total threads\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} total threads\".format(len(master.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Check how many comments I have total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5724 total comments\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for k,v in master.items():\n",
    "    i += len(v)\n",
    "print(\"There are {} total comments\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, I can compare the number of actual comments scraped with the expected number of comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"Gank Timers\" &amp; Champion pool help | Actual: 14 Expected: 14',\n",
       " '\"I\\'m Just a bit Rusty\" Losing Streak | Actual: 3 Expected: 3',\n",
       " '\"Janna is bad unless you are a Pro\" | Actual: 141 Expected: 142',\n",
       " '\"Reliable\" champs like Wu, suggestions? | Actual: 10 Expected: 10',\n",
       " '(Ezreal) Roadblocks on the Search for an S | Actual: 18 Expected: 18',\n",
       " '**How to Shotcall?** | Actual: 12 Expected: 12',\n",
       " '10 Rune Pages - Requesting input on pages. | Actual: 5 Expected: 5',\n",
       " '100k Subs Celebration: INFOGRAPHIC DESIGN CONTEST!! | Actual: 5 Expected: 17',\n",
       " '11 Thresh Questions | Actual: 17 Expected: 17',\n",
       " \"17% win rate with Jhin. I love the champion but I'm not so sure if I should be playing him anymore. | Actual: 19 Expected: 19\",\n",
       " '30+ year old players out there? Share your game xp! | Actual: 21 Expected: 21',\n",
       " '3v3 tactic? | Actual: 1 Expected: 1',\n",
       " '5 Tips To Get More Jungle Ganks | Actual: 50 Expected: 51',\n",
       " '5 man bottom | Actual: 10 Expected: 10',\n",
       " '50 Shades of Bronze. | Actual: 166 Expected: 166',\n",
       " 'A (Diamond/ex-challenger) Akali-One-Tricks First Guide to Akali Mid With Diamond Gameplay | Actual: 48 Expected: 48',\n",
       " 'A (Diamond/ex-challenger) Akali-One-Tricks First Guide to Akali, The Ultimate Yasuo Counter | Actual: 62 Expected: 62',\n",
       " 'A Bunch of Yasuo Questions | Actual: 9 Expected: 9',\n",
       " 'A Diamond Support Coaches a Silver/Gold ADC ... by ioki | Actual: 2 Expected: 2',\n",
       " 'A Silver 4 Player Looking to Climb to Gold before the season ends | Actual: 8 Expected: 8'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k+\" | Actual: \"+str(v['actual_comments'])+\" Expected: \"+str(v['expected_comments']) for k,v in sorted(master.items())[0:20]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The counts are mostly right, some are a bit off. Couple of reasons:\n",
    "\n",
    "1. Sometimes more comments are posted after I pull the intial count.\n",
    "2. If a thread is especially long, the comments are stored in a \"continue thread\" link, which I'm not particularly interested in.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save file\n",
    "***\n",
    "Finally, I'll save the text data just so I don't have to run this again unless I want more data. I'm going to dump it into a json with just the thread name and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save = {}\n",
    "for k,v in master.items():\n",
    "    save.setdefault(k,v['comments'])\n",
    "\n",
    "json.dump(save,open('thread_comments_100pg.txt','w'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
